# Training settings
[GENERAL]
    # Number of training steps per epoch
    # This doesn't need to match the size of the training set. Tensorboard
    # updates are saved at the end of each epoch, so setting this to a
    # smaller number means getting more frequent TensorBoard updates.
    # Validation stats are also calculated at each epoch end and they
    # might take a while, so don't set this too small to avoid spending
    # a lot of time on validation stats.
    int-STEPS_PER_EPOCH = 200

    # Number of total training epochs
    int-EPOCHS = 5

    # Number of validation steps to run at the end of every training epoch.
    # A bigger number improves accuracy of validation stats, but slows
    # down the training.
    int-VALIDATION_STEPS = 20

    # Learning rate and momentum
    float-LEARNING_RATE = 0.00005
    float-LEARNING_MOMENTUM = 0.9

[LAYERS]
    # Train or freeze batch normalization layers
    bool-TRAIN_BN = True

    # Trainable layers
    # To make all layers trainable, use: str-TRAINABLE_LAYERS = all
    # Otherwise, use: list-TRAINABLE_LAYERS = [list of double quoted strings]
    # Examples of possible strings: "backbone", "fpn", "rpn", "mrcnn"
    # If "backbone" is specified, the trainer will make all layers in backbone trainable.
    # Basically, if you specify other string, say "foo", the trainer will make all layers with the name beginning with `foo_`
    # In particular, if you are using one of resnets as backbone model,
    # you can specify the stages of backbone layers you want to train instead of just including "backbone"
    # For example: list-TRAINABLE_LAYERS = ["stage3", "stage4", "fpn", "rpn"]
    list-TRAINABLE_LAYERS = ["pyramid", "segm", "conv_", "conv2d_"]

[LOSS]
    # Weight decay for l2 regularization
	# Set this value so that regularization loss is about in the similar range with other losses
    float-WEIGHT_DECAY = 1.0

    # Loss weights for more precise optimization.
    dict-LOSS_WEIGHTS = {"cce_loss": 1.0, "jaccard_loss": 1.0, "dice_loss": 1.0}

[OPTIMIZER]
    # Optimizer
    # Possible values:
    str-OPTIMIZER = Adam

    # Gradient norm clipping
    float-GRADIENT_CLIP_NORM = 5.0

[SGD]
    # Whether to use nesterov
    # This is going to be used only when OPTIMIZER is SGD
    # Otherwise this value is meaningless
    bool-NESTEROV = True

[Adam]
    # Whether to use amsgrad
    # This is going to be used only when OPTIMIZER is Adam
    # Otherwise this value is meaningless
    bool-AMSGRAD = True

